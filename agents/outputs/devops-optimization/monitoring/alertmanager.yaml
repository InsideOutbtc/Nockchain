apiVersion: apps/v1
kind: Deployment
metadata:
  name: alertmanager
  namespace: monitoring
  labels:
    app: alertmanager
spec:
  replicas: 3
  selector:
    matchLabels:
      app: alertmanager
  template:
    metadata:
      labels:
        app: alertmanager
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - alertmanager
              topologyKey: kubernetes.io/hostname
      containers:
      - name: alertmanager
        image: prom/alertmanager:v0.25.0
        ports:
        - containerPort: 9093
        args:
        - --config.file=/etc/alertmanager/alertmanager.yml
        - --storage.path=/alertmanager
        - --web.external-url=https://alertmanager.nockchain.com
        - --cluster.listen-address=0.0.0.0:9094
        - --cluster.advertise-address=$(POD_IP):9094
        - --log.level=info
        - --data.retention=120h
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        volumeMounts:
        - name: config
          mountPath: /etc/alertmanager
        - name: storage
          mountPath: /alertmanager
        - name: templates
          mountPath: /etc/alertmanager/templates
        livenessProbe:
          httpGet:
            path: /-/healthy
            port: 9093
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /-/ready
            port: 9093
          initialDelaySeconds: 5
          periodSeconds: 5
      volumes:
      - name: config
        configMap:
          name: alertmanager-config
      - name: templates
        configMap:
          name: alertmanager-templates
      - name: storage
        persistentVolumeClaim:
          claimName: alertmanager-storage
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: alertmanager-storage
  namespace: monitoring
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: high-performance-ssd
  resources:
    requests:
      storage: 10Gi
---
apiVersion: v1
kind: Service
metadata:
  name: alertmanager
  namespace: monitoring
spec:
  selector:
    app: alertmanager
  ports:
  - port: 9093
    targetPort: 9093
    name: web
  - port: 9094
    targetPort: 9094
    name: cluster
  type: ClusterIP
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: monitoring
data:
  alertmanager.yml: |
    global:
      smtp_smarthost: 'smtp.gmail.com:587'
      smtp_from: 'alerts@nockchain.com'
      smtp_auth_username: 'alerts@nockchain.com'
      smtp_auth_password: '${SMTP_PASSWORD}'
      slack_api_url: '${SLACK_API_URL}'
      pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'
      
    templates:
    - '/etc/alertmanager/templates/*.tmpl'
    
    route:
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 1h
      receiver: 'default-receiver'
      routes:
      - match:
          severity: critical
        receiver: 'critical-alerts'
        group_wait: 10s
        group_interval: 5m
        repeat_interval: 30m
      - match:
          severity: warning
        receiver: 'warning-alerts'
        group_wait: 30s
        group_interval: 10m
        repeat_interval: 2h
      - match:
          alertname: ServiceDown
        receiver: 'service-down-alerts'
        group_wait: 0s
        group_interval: 1m
        repeat_interval: 5m
      - match:
          alertname: HighResponseTime
        receiver: 'performance-alerts'
        group_wait: 30s
        group_interval: 5m
        repeat_interval: 1h
      - match:
          alertname: DatabaseConnectionExhaustion
        receiver: 'database-alerts'
        group_wait: 10s
        group_interval: 2m
        repeat_interval: 30m
    
    receivers:
    - name: 'default-receiver'
      email_configs:
      - to: 'devops-team@nockchain.com'
        subject: '[Nockchain] Alert: {{ .GroupLabels.alertname }}'
        body: |
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Instance: {{ .Labels.instance }}
          Severity: {{ .Labels.severity }}
          Time: {{ .StartsAt.Format "2006-01-02 15:04:05" }}
          {{ end }}
    
    - name: 'critical-alerts'
      pagerduty_configs:
      - routing_key: '${PAGERDUTY_ROUTING_KEY}'
        description: 'Critical Alert: {{ .GroupLabels.alertname }}'
        details:
          summary: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
          description: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
          severity: 'critical'
          timestamp: '{{ .Alerts.0.StartsAt.Format "2006-01-02T15:04:05Z" }}'
      slack_configs:
      - channel: '#critical-alerts'
        title: 'CRITICAL: {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          üö® *{{ .Annotations.summary }}*
          
          *Description:* {{ .Annotations.description }}
          *Instance:* {{ .Labels.instance }}
          *Time:* {{ .StartsAt.Format "2006-01-02 15:04:05" }}
          {{ end }}
        color: danger
        send_resolved: true
      email_configs:
      - to: 'critical-alerts@nockchain.com'
        subject: '[CRITICAL] {{ .GroupLabels.alertname }}'
        body: |
          CRITICAL ALERT TRIGGERED
          
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Instance: {{ .Labels.instance }}
          Time: {{ .StartsAt.Format "2006-01-02 15:04:05" }}
          
          {{ end }}
          
          Please investigate immediately.
    
    - name: 'warning-alerts'
      slack_configs:
      - channel: '#alerts'
        title: 'WARNING: {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          ‚ö†Ô∏è *{{ .Annotations.summary }}*
          
          *Description:* {{ .Annotations.description }}
          *Instance:* {{ .Labels.instance }}
          *Time:* {{ .StartsAt.Format "2006-01-02 15:04:05" }}
          {{ end }}
        color: warning
        send_resolved: true
      email_configs:
      - to: 'warnings@nockchain.com'
        subject: '[WARNING] {{ .GroupLabels.alertname }}'
        body: |
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Instance: {{ .Labels.instance }}
          Time: {{ .StartsAt.Format "2006-01-02 15:04:05" }}
          {{ end }}
    
    - name: 'service-down-alerts'
      pagerduty_configs:
      - routing_key: '${PAGERDUTY_ROUTING_KEY}'
        description: 'Service Down: {{ .GroupLabels.alertname }}'
        details:
          summary: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
          description: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
          severity: 'critical'
      slack_configs:
      - channel: '#service-down'
        title: 'SERVICE DOWN: {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          üî¥ *SERVICE DOWN*
          
          *Service:* {{ .Labels.job }}
          *Instance:* {{ .Labels.instance }}
          *Time:* {{ .StartsAt.Format "2006-01-02 15:04:05" }}
          {{ end }}
        color: danger
        send_resolved: true
      webhook_configs:
      - url: 'https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX'
        send_resolved: true
    
    - name: 'performance-alerts'
      slack_configs:
      - channel: '#performance'
        title: 'Performance Alert: {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          üìä *{{ .Annotations.summary }}*
          
          *Description:* {{ .Annotations.description }}
          *Instance:* {{ .Labels.instance }}
          *Time:* {{ .StartsAt.Format "2006-01-02 15:04:05" }}
          {{ end }}
        color: warning
        send_resolved: true
      email_configs:
      - to: 'performance-team@nockchain.com'
        subject: '[PERFORMANCE] {{ .GroupLabels.alertname }}'
        body: |
          {{ range .Alerts }}
          Performance Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Instance: {{ .Labels.instance }}
          Time: {{ .StartsAt.Format "2006-01-02 15:04:05" }}
          {{ end }}
    
    - name: 'database-alerts'
      pagerduty_configs:
      - routing_key: '${PAGERDUTY_DATABASE_KEY}'
        description: 'Database Alert: {{ .GroupLabels.alertname }}'
        details:
          summary: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
          description: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
          severity: 'critical'
      slack_configs:
      - channel: '#database-alerts'
        title: 'DATABASE ALERT: {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          üóÑÔ∏è *{{ .Annotations.summary }}*
          
          *Description:* {{ .Annotations.description }}
          *Instance:* {{ .Labels.instance }}
          *Time:* {{ .StartsAt.Format "2006-01-02 15:04:05" }}
          {{ end }}
        color: danger
        send_resolved: true
      email_configs:
      - to: 'database-team@nockchain.com'
        subject: '[DATABASE] {{ .GroupLabels.alertname }}'
        body: |
          {{ range .Alerts }}
          Database Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Instance: {{ .Labels.instance }}
          Time: {{ .StartsAt.Format "2006-01-02 15:04:05" }}
          {{ end }}
    
    inhibit_rules:
    - source_match:
        severity: 'critical'
      target_match:
        severity: 'warning'
      equal: ['alertname', 'cluster', 'service']
    
    - source_match:
        alertname: 'ServiceDown'
      target_match_re:
        alertname: 'High.*'
      equal: ['instance']
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-templates
  namespace: monitoring
data:
  default.tmpl: |
    {{ define "slack.default.title" }}
    {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}
    {{ end }}
    
    {{ define "slack.default.text" }}
    {{ range .Alerts }}
    *Description:* {{ .Annotations.description }}
    *Instance:* {{ .Labels.instance }}
    *Severity:* {{ .Labels.severity }}
    *Time:* {{ .StartsAt.Format "2006-01-02 15:04:05" }}
    {{ end }}
    {{ end }}
    
    {{ define "email.default.subject" }}
    [{{ .Status | toUpper }}] {{ .GroupLabels.alertname }}
    {{ end }}
    
    {{ define "email.default.html" }}
    <h2>{{ .Status | toUpper }}</h2>
    <h3>{{ .GroupLabels.alertname }}</h3>
    <table>
      <thead>
        <tr>
          <th>Alert</th>
          <th>Description</th>
          <th>Instance</th>
          <th>Severity</th>
          <th>Time</th>
        </tr>
      </thead>
      <tbody>
        {{ range .Alerts }}
        <tr>
          <td>{{ .Annotations.summary }}</td>
          <td>{{ .Annotations.description }}</td>
          <td>{{ .Labels.instance }}</td>
          <td>{{ .Labels.severity }}</td>
          <td>{{ .StartsAt.Format "2006-01-02 15:04:05" }}</td>
        </tr>
        {{ end }}
      </tbody>
    </table>
    {{ end }}